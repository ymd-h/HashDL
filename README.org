#+options: ':nil *:t -:t ::t <:t H:3 \n:nil ^:t arch:headline
#+options: author:t broken-links:nil c:nil creator:nil
#+options: d:(not "LOGBOOK") date:t e:t email:nil f:t inline:t num:t
#+options: p:nil pri:nil prop:nil stat:t tags:t tasks:t tex:t
#+options: timestamp:t title:t toc:t todo:t |:t
#+title: HashDL (Hash-based Deep Learning)
#+date: <2021-05-28 Fri>
#+author: Hiroyuki Yamada
#+language: en
#+select_tags: export
#+exclude_tags: noexport
#+creator: Emacs 27.1 (Org mode 9.3.7)


* Overview
This repository is non-official third-paty re-implementation of SLIDE[fn:1].

We provide
- Python package
- Hash based Deep Learning
- Parallel computing based on C++17 parallel STL


We don't provide
- Explicit CPU optimized code like AVX (We just rely on compiler optimization)
- Compiled binary (You need to compile by yourself)


* Install

There are two options, "Install from PyPI" and "Install from Source".
For ordinary user, "Install from PyPI" is recommended.

For both case, sufficient C++ compiler is neccessary.

** Requirement
- Recent C++ compiler with parallel STL algorithm support
  - [[https://gcc.gnu.org/][GCC]] 9.1 or newer together with [[https://github.com/oneapi-src/oneTBB][Intel TBB]]
- [[https://www.python.org/][Python]] 3


Requirements can be installed on Docker image [[https://hub.docker.com/_/gcc][gcc:10]].

#+begin_src shell
# On local machine
docker run -it gcc:10 bash

# On gcc:10 image
apt update && apt install -y python3-pip libtbb-dev
#+end_src


** Install from PyPI

#+begin_src shell
pip install HashDL
#+end_src


** Install from Source

#+begin_src shell
git clone https://gitlab.com/ymd_h/hashdl.git HashDL
cd HashDL
pip install .
#+end_src

* Features

- Neural Network
  - hash-based sparse dense layer
- Activation
  - ReLU
  - linear (no activation)
  - sigmoid
- Optimizer
  - SGD
  - Adam[fn:3]
- Weight Initializer
  - constant
  - Gauss distribution
- Hash for similarity
  - WTA
  - DWTA[fn:2]
- Scheduler for hash update
  - constant
  - exponential decay


In the current architecture, CNN is impossible.

* Implementation

The [[https://github.com/keroro824/HashingDeepLearning][official reference implementation]] focused on performance and
accepted some "dirtyness" like hard-coded magic number for algotihm
selection and unmanaged memory allocation.

We accept some (but hopefully small) overhead and improve
maintenability in terms of software;

- Polymorphism with inheritance and virtual function
- RAII and smart pointer for memory management

These archtecture allows us to construct and manage C++ class from
Python without recompile.


We also rely recent C++ standard and compiler optimization;

- Parallel STL from C++17
- Because of RVO (or at least move semantics), returning ~std::vector~
  is not so much costful as it was.


* Footnotes

[fn:3] [[https://iclr.cc/archive/www/doku.php%3Fid=iclr2015:main.html][D. P. Kingma and J. Ba, "Adam: A Method for Stochastic Optimization", ICLR (2015)]] ([[https://arxiv.org/abs/1412.6980][arXiv]])

[fn:2] [[http://auai.org/uai2018/proceedings/papers/321.pdf][B. Chen /et al/., "Densified Winner Take All (WTA) Hashing for Sparse Datasets", Uncertainty in artificial intelligence (2018)]]

[fn:1] [[https://mlsys.org/Conferences/2020/Schedule?showEvent=1410][B. Chen /et al/., "SLIDE : In Defense of Smart Algorithms over Hardware Acceleration for Large-Scale Deep Learning Systems", MLSys 2020]] ([[https://arxiv.org/abs/1903.03129][arXiv]], [[https://github.com/keroro824/HashingDeepLearning][code]])
